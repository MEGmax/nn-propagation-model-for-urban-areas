# Model Assessment & Backtesting Summary

**Date**: January 29, 2025  
**Project**: Conditional Diffusion Model for Radio Propagation Maps  
---

## üìä What Was Created

### 1. **Comprehensive Backtesting Framework** (`backtest_evaluation.py`)
- ‚úì RadioMapEvaluator: Computes 15+ quality metrics per scene
- ‚úì RSSNormalizer: Handles [-1, 1] ‚Üî dBm conversions
- ‚úì DiffusionSampler: Generates ensemble predictions
- ‚úì Batch evaluation with aggregation
- ‚úì JSON export of detailed results
- ‚úì Diagnostic plots (RMSE/MAE histograms, error CDF, bias distribution)

**Metrics Computed**:
```
- RMSE (dB)              [Target: < 5 dB]
- MAE (dB)               [Target: < 5 dB]
- Median Error (dB)      [Target: < 3 dB]
- Bias (dB)              [Target: ‚âà 0 dB]
- Coverage @ 3/5/10 dB   [Target: >40/60/80%]
- Pearson Correlation    [Target: > 0.7]
- Per-percentile errors  [25th, 50th, 75th, 90th]
```

---

### 2. **Improved Training Script** (`models/model.py`)
**New Features**:
- Checkpoint manager with metadata tracking
- Resume-from-checkpoint capability
- Detailed logging at each epoch
- CLI argument parsing (epochs, batch size, learning rate, etc.)
- GPU memory profiling
- Training summary report

**Usage**:
```bash
cd models
python model.py \
    --epochs 50 \
    --batch-size 4 \
    --lr 2e-4 \
    --save-every 5 \
    --resume
```

---

### 3. **Quick-Start Evaluation** (`run_backtest.py`)
**Features**:
- One-command evaluation: `python run_backtest.py`
- --quick flag for fast testing (3 samples, 20 diffusion steps)
- Custom checkpoint support
- Automatic result visualization & reporting

**Usage**:
```bash
# Full evaluation
python run_backtest.py

# Quick test
python run_backtest.py --quick

# Custom checkpoint
python run_backtest.py --checkpoint models/checkpoints/model_epoch30.pt
```

---

### 4. **Comprehensive Guides**

#### A. `EVALUATION_GUIDE.md`
- Metric definitions & interpretation
- Expected performance levels (baseline ‚Üí well-trained)
- Installation & data structure
- Diagnostic tips for common issues
- Model architecture reference
- Advanced usage examples

#### B. `AIRMAP_COMPARISON.md`
- Side-by-side architecture comparison
- Input conditioning analysis (your 3 channels vs AIRMap's 1)
- Training data gap analysis (5 vs 60,000 samples)
- Scaling law predictions
- Performance expectations (15-25 dB RMSE at 5 scenes)
- Optimization roadmap (4 phases)
- Realistic next steps for reaching AIRMap-level performance

---

## üéØ Key Findings

### Your Model vs AIRMap
| Aspect | Your Model | AIRMap | Status |
|--------|-----------|--------|--------|
| **Architecture** | Diffusion | U-Net | ‚úì Novel approach |
| **Inputs** | Elevation + distance + freq | Elevation only | ‚úì Richer conditioning |
| **Training data** | 5 scenes | 60,000 scenes | ‚ö† **12,000x gap** |
| **Expected RMSE** | 15-25 dB | < 5 dB | ‚ö† Need more data |
| **Uncertainty** | Yes (stochastic) | No | ‚úì Your advantage |
| **Inference speed** | 0.5-2 sec | 4 ms | ‚ö† Trade-off for uncertainty |

### Performance Expectations

**With 5 training scenes**, realistically:
```
RMSE:           15-25 dB (not a failure, normal at this scale)
MAE:            12-20 dB
Bias:           ¬±5 dB (calibration offset expected)
Coverage (5dB): 20-40% (spatial accuracy is limited)
Coverage (10dB):60-80%
```

**This is not competitive with AIRMap yet, but:**
- Your diffusion approach provides **uncertainty quantification**
- You're learning from **richer conditioning** (distance + frequency)
- **Scaling to 100+ scenes** will show dramatic improvement
- Proper **calibration with field data** can reduce real-world error by 50-70%

---

## üöÄ Immediate Next Steps

### Step 1: Run Backtesting (Today)
```bash
cd /Users/matthewgrech/ECE2T5F/ECE496/Diffusion

# Train briefly (establish baseline)
cd models && python model.py --epochs 5 --batch-size 2
cd ..

# Evaluate
python run_backtest.py --quick
```

**Expected Output**:
- `backtest_results/backtest_results.json` (detailed metrics)
- `backtest_results/backtest_metrics.png` (diagnostic plots)
- Console report with RMSE/MAE/coverage stats

### Step 2: Analyze Failure Modes (This Week)
1. Plot predicted vs ground truth (side-by-side heatmaps)
2. Check error distribution across spatial locations
3. Verify model is learning (training loss decreasing?)
4. Look for NaN/Inf values or numerical instability

### Step 3: Increase Data (Next 2 Weeks)
1. Generate 10-20 additional scenes using `scene_generation/`
2. Create train/validation/test split
3. Re-train and re-evaluate
4. Plot RMSE vs dataset size (should decrease)

### Step 4: Optimize Model (Ongoing)
- Add material map as 4th conditioning channel (if available)
- Try larger model: `base_ch=64, channel_mults=(1,2,4,8)`
- Experiment with diffusion schedules (cosine, sqrt)
- Implement ensemble inference (average 10+ samples)

---

## üìà Success Criteria

### Minimum Viability
- [ ] Model trains without errors
- [ ] Loss decreases over epochs
- [ ] RMSE < 30 dB on training set
- [ ] No NaN/Inf in predictions

### Good Progress
- [ ] RMSE < 15 dB
- [ ] Coverage (5 dB) > 40%
- [ ] Bias < ¬±2 dB
- [ ] Spatial correlation > 0.5

### Competitive with AIRMap
- [ ] RMSE < 7 dB
- [ ] Coverage (5 dB) > 70%
- [ ] Requires ~1000 training scenes
- [ ] Field measurement calibration applied

---

## üìö Key Documents

| File | Purpose |
|------|---------|
| `backtest_evaluation.py` | Full evaluation framework & metrics |
| `run_backtest.py` | Quick-start evaluation script |
| `models/model.py` | Improved training with checkpointing |
| `EVALUATION_GUIDE.md` | Metric definitions & interpretation |
| `AIRMAP_COMPARISON.md` | Detailed comparison & optimization roadmap |
| `TENSOR_SHAPES_REFERENCE.md` | Shape conventions & normalization |

---

## üîç Backtesting Workflow

```
1. Train Model (models/model.py)
   ‚îî‚îÄ> Saves checkpoints to models/checkpoints/

2. Run Evaluation (run_backtest.py)
   ‚îú‚îÄ> Loads checkpoint
   ‚îú‚îÄ> Generates ensemble samples (5 per input)
   ‚îú‚îÄ> Computes 15+ metrics per scene
   ‚îî‚îÄ> Aggregates results

3. Analyze Results (backtest_results/)
   ‚îú‚îÄ> backtest_results.json (raw data)
   ‚îú‚îÄ> backtest_metrics.png (plots)
   ‚îî‚îÄ> Console report (summary)

4. Iterate
   ‚îú‚îÄ> Identify failure modes
   ‚îú‚îÄ> Add more training data
   ‚îú‚îÄ> Adjust model architecture
   ‚îî‚îÄ> Repeat steps 1-3
```

---

## üí° Important Insights

### Why Your Model Won't Match AIRMap Yet (And Why That's OK)
1. **Data Scale**: 12,000x fewer training samples
2. **Architectural Overhead**: Diffusion adds complexity for uncertainty
3. **Generalization**: 5 scenes can't cover diverse urban geometry

### Your Model's Unique Strengths
1. **Stochastic Predictions**: Get uncertainty estimates (pixel-wise variance)
2. **Richer Conditioning**: Uses distance + frequency (path loss factors)
3. **Ensemble Learning**: Average multiple samples for confidence

### Path to Competitiveness
1. **Short-term**: Focus on data generation (aim for 100+ scenes)
2. **Medium-term**: Implement calibration pipeline with field data
3. **Long-term**: Demonstrate value of uncertainty quantification

---

## ‚ö†Ô∏è Common Pitfalls

| Issue | Symptom | Fix |
|-------|---------|-----|
| **Overfitting** | Train RMSE: 3dB, Test RMSE: 25dB | Add scenes, use validation set |
| **NaN Loss** | Loss = NaN after 10 epochs | Check data normalization, reduce LR |
| **Poor Spatial Accuracy** | Correlation ‚âà 0 | Improve condition encoder (`cond_proj`) |
| **Slow Training** | 5 min/epoch | Reduce batch size, use fewer workers |
| **High Bias** | Predictions consistently 5dB too high | Recalibrate, check target normalization |

---

## üéì References

1. **AIRMap Paper** (arXiv:2511.05522):
   - Saeizadeh et al., "AIRMap -- AI-Generated Radio Maps for Wireless Digital Twins"
   - <https://arxiv.org/abs/2511.05522>

2. **Diffusion Models**:
   - Ho et al., "Denoising Diffusion Probabilistic Models" (DDPM)
   - <https://arxiv.org/abs/2006.11239>

3. **Your Codebase**:
   - `models/diffusion.py`: Core model architecture
   - `models/model.py`: Training entry point
   - `model_input/model_input.py`: Data pipeline
   - `copilot-instructions.md`: Project conventions

---

## ‚úÖ Checklist: Ready to Backtest?

- [ ] Python environment configured (`.venv/bin/python`)
- [ ] Dataset loaded (5 scenes in `model_input/data/`)
- [ ] Model checkpoint exists or you can train new one
- [ ] Read `EVALUATION_GUIDE.md` for metric interpretation
- [ ] Read `AIRMAP_COMPARISON.md` for expectations
- [ ] Run: `python run_backtest.py --quick` for 5-min test
- [ ] Run: `python run_backtest.py` for full evaluation

---

## üìû Next Steps

1. **Now**: Review this document and the two guides
2. **Today**: Run `python run_backtest.py --quick`
3. **This week**: Generate more training scenes, re-evaluate
4. **This month**: Reach 50+ scenes, implement proper train/val/test split

---

**Status**: ‚úÖ Framework Ready  
**Last Updated**: 2025-01-29  
**Assessment Stage**: Pre-baseline (awaiting first backtest results)
