# Quick Reference: Backtesting Your Model

## ðŸš€ Run Backtesting in 2 Commands

```bash
# Full evaluation (~10 minutes)
python run_backtest.py

# Quick test (~2 minutes)
python run_backtest.py --quick
```

## ðŸ“Š What You'll Get

| Output | Location | Purpose |
|--------|----------|---------|
| Metrics JSON | `backtest_results/backtest_results.json` | Raw data: RMSE, MAE, bias, coverage |
| Plots | `backtest_results/backtest_metrics.png` | Visual: RMSE hist, error CDF, bias |
| Console Report | Printed to terminal | Summary: mean/std/min/max across scenes |

## ðŸ“ˆ Key Metrics Explained

### Primary (Most Important)

```
RMSE (dB)
â”œâ”€ What: Root mean squared error in dB
â”œâ”€ Your target: < 10 dB (good), < 20 dB (okay)
â”œâ”€ AIRMap achieves: < 5 dB
â””â”€ âœ“ Lower is better

MAE (dB)
â”œâ”€ What: Average absolute error
â”œâ”€ Similar interpretation to RMSE
â””â”€ Usually slightly lower than RMSE

Coverage (% within threshold)
â”œâ”€ "within 5 dB": % of pixels with |error| â‰¤ 5 dB
â”œâ”€ Your target: > 60% (good)
â”œâ”€ AIRMap achieves: > 70%
â””â”€ âœ“ Higher is better
```

### Secondary (Supporting Info)

```
Bias (dB)
â”œâ”€ What: Systematic over/under-prediction
â”œâ”€ Ideal: 0 dB (well-calibrated)
â”œâ”€ Â±2-5 dB: Normal, can be corrected
â””â”€ > Â±10 dB: Data/model issue

Median Error (dB)
â”œâ”€ What: 50th percentile (robust to outliers)
â”œâ”€ Usually better than RMSE for skewed distributions
â””â”€ More stable metric at small sample size

Pearson Correlation
â”œâ”€ What: Does prediction track ground truth spatially?
â”œâ”€ Range: -1 (inverse) to +1 (perfect)
â”œâ”€ Target: > 0.7 (good spatial structure)
â””â”€ < 0.5: Model not learning conditioning
```

## âœ… Success Interpretation

```
RMSE < 10 dB      âœ“ Model is learning well
                  â†’ Spatial patterns captured
                  â†’ Realistic error magnitude

RMSE 10-20 dB     âš  Decent for early development
                  â†’ Need more training data
                  â†’ Or improve model architecture

RMSE > 20 dB      âš  Needs improvement
                  â†’ Check data normalization
                  â†’ Verify model training
                  â†’ Could be just scale (5 scenes)

Coverage > 70%    âœ“ Excellent spatial accuracy
@ 5 dB

Coverage < 40%    âš  Model is uncertain/variable
@ 5 dB            â†’ Small dataset limitation
                  â†’ Expected at 5 scenes

Bias = 0 dB       âœ“ Perfect calibration
                  (unlikely but great if true)

Bias Â± 5 dB       âš  Systematic offset
                  â†’ Can be corrected post-hoc
                  â†’ Not a major issue

Bias > Â± 10 dB    âœ— Significant problem
                  â†’ Check normalization
                  â†’ Verify loss function
```

## ðŸŽ¯ Performance by Stage

### Baseline (Your Current: 5 Scenes)
```
Expected RMSE:    15-25 dB
Expected MAE:     12-18 dB
Expected Coverage (5dB): 20-40%
Interpretation:   NORMAL - lots of room to improve
```

### Good Progress (50 Scenes)
```
Expected RMSE:    10-15 dB
Expected MAE:     8-12 dB
Expected Coverage (5dB): 40-60%
Interpretation:   Learning working, needs more data
```

### Competitive (500 Scenes)
```
Expected RMSE:    7-10 dB
Expected MAE:     5-8 dB
Expected Coverage (5dB): 65-80%
Interpretation:   Approaching AIRMap range
```

### State-of-Art (2000+ Scenes)
```
Expected RMSE:    < 5 dB âœ“
Expected MAE:     3-5 dB âœ“
Expected Coverage (5dB): > 80% âœ“
Interpretation:   Competitive with AIRMap
```

## ðŸ”§ Quick Troubleshooting

| Problem | Check | Solution |
|---------|-------|----------|
| RMSE > 25 dB | Is model trained? | Run `python models/model.py` first |
| NaN in metrics | Data normalization | Check input ranges |
| Very high bias | Target centering | Verify RSS normalization in model_input.py |
| Low correlation | Conditioning | Check if model receiving inputs correctly |
| Out of memory | Batch size | Reduce from 2 to 1 |
| Slow evaluation | Num samples | Use `--quick` flag or reduce `--samples-per-scene` |

## ðŸ“Š Metric Formulas

```
RMSE = sqrt(mean((predicted - ground_truth)Â²))

MAE = mean(|predicted - ground_truth|)

Bias = mean(predicted - ground_truth)  [signed, can be negative]

Coverage @ Xdb = (# pixels with |error| â‰¤ X) / total_pixels

Pearson Correlation = cov(predicted, ground_truth) / (std_pred Ã— std_gt)
```

## ðŸ”‘ What Each Plot Means

### RMSE Distribution Histogram
- **Left peak**: Easier scenes (lower error)
- **Right tail**: Harder scenes (higher error)
- **Ideal**: Narrow, centered on target RMSE

### Error CDF (Cumulative Distribution Function)
- **Y-axis**: % of pixels below error threshold
- **Steep line**: Most predictions concentrated in narrow error range (good)
- **Flat line**: Errors spread across wide range (bad)
- **Cross @ 5dB**: Coverage threshold visualization

### Bias Distribution
- **Centered on 0**: Model is well-calibrated âœ“
- **Shifted left**: Model over-predicts (bias < 0)
- **Shifted right**: Model under-predicts (bias > 0)

## ðŸŽ“ Real Numbers from AIRMap Paper

As a benchmark:
```
AIRMap (60k Boston scenes):
â”œâ”€ RMSE: 4.8 dB
â”œâ”€ MAE: 3.2 dB
â”œâ”€ Median: 2.1 dB
â”œâ”€ Coverage (5dB): 84%
â””â”€ Inference: 4 ms per map

Your Model Target (1000 scenes):
â”œâ”€ RMSE: 5-7 dB (reasonable)
â”œâ”€ MAE: 4-5 dB (reasonable)
â”œâ”€ Coverage (5dB): 70% (reasonable)
â””â”€ Inference: 0.5-2 sec (acceptable for uncertainty)
```

## ðŸ“‹ Backtest Workflow

```
1. Train model (optional)
   python models/model.py --epochs 10

2. Run evaluation
   python run_backtest.py --quick

3. Check results
   cat backtest_results/backtest_results.json | head -50
   open backtest_results/backtest_metrics.png

4. Iterate
   - If RMSE > 20: train longer or add data
   - If RMSE 10-20: collect more scenes
   - If RMSE < 10: try model improvements
```

## ðŸš¨ Red Flags

```
âœ— RMSE = NaN               â†’ Data problem, stop
âœ— RMSE = Inf               â†’ Numerical overflow, stop
âœ— Loss increases w/ epochs â†’ Learning rate too high
âœ— Coverage = 0% @ 10dB     â†’ Model completely broken
âœ— Correlation < 0          â†’ Inverted predictions?
âœ— Loss = NaN after epoch 3 â†’ Data corruption
```

## âœ… Green Lights

```
âœ“ Loss decreasing         â†’ Training working
âœ“ RMSE improving w/ data  â†’ Scaling correctly
âœ“ Bias â‰ˆ Â±2 dB            â†’ Well-calibrated
âœ“ Correlation > 0.5       â†’ Spatial learning
âœ“ Coverage 40%+ @ 5dB     â†’ Reasonable accuracy
âœ“ Per-scene consistency   â†’ Model is stable
```

---

## ðŸŽ¯ Your Action Items

### Today (5 min)
- [ ] Read this document
- [ ] Run: `python run_backtest.py --quick`
- [ ] Check console output for errors

### This Week (1-2 hours)
- [ ] Run: `python run_backtest.py` (full evaluation)
- [ ] Read the generated JSON results
- [ ] Analyze backtest_metrics.png
- [ ] Compare RMSE to expectations above

### Next 2 Weeks (ongoing)
- [ ] Generate 10+ more training scenes
- [ ] Re-run backtesting with more data
- [ ] Plot RMSE vs dataset size (should decrease)
- [ ] Identify spatial failure patterns

---

**Last Updated**: 2025-01-29  
**Status**: Ready to Run
